{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Occupy a GPU for the model to be loaded \n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# GPU ID, if occupied change to an available GPU ID listed under !nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import Chem, DataStructs\n",
    "import h5py, ast, pickle\n",
    "\n",
    "from ddc_pub import ddc_v3 as ddc\n",
    "from ddc_pub.vectorizers import SmilesVectorizer\n",
    "\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/OPD_Data/FP_C_Training_Validation.csv')['smiles'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binmols = [(Chem.MolFromSmiles(smiles)) for smiles in data]\n",
    "mols_in = [Chem.rdchem.Mol.ToBinary(smiles) for smiles in binmols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SmilesVectorizer()\n",
    "sv.fit(binmols)\n",
    "maxlen = sv.maxlength\n",
    "charset = sv.charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset\n",
    "name = \"OPD_FP_C\"\n",
    "\n",
    "dataset_info = {\"charset\": charset, \"maxlen\": maxlen + 35, \"name\": name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model in train mode.\n",
      "Input type is 'binary mols'.\n",
      "Model received 141898 train samples and 15767 validation samples.\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_Inputs (InputLayer)     [(None, 460, 67)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mol_to_latent_model (Model)     (None, 128)          733824      Encoder_Inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Inputs (InputLayer)     [(None, 459, 67)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "latent_to_states_model (Model)  [(None, 256), (None, 204288      mol_to_latent_model[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_model (Model)             (None, 459, 67)      1405763     Decoder_Inputs[0][0]             \n",
      "                                                                 latent_to_states_model[1][0]     \n",
      "                                                                 latent_to_states_model[1][1]     \n",
      "                                                                 latent_to_states_model[1][2]     \n",
      "                                                                 latent_to_states_model[1][3]     \n",
      "                                                                 latent_to_states_model[1][4]     \n",
      "                                                                 latent_to_states_model[1][5]     \n",
      "==================================================================================================\n",
      "Total params: 2,343,875\n",
      "Trainable params: 2,336,451\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = ddc.DDC(x              = mols_in,\n",
    "                y              = mols_in,\n",
    "                dataset_info   = dataset_info,\n",
    "                noise_std      = 0.1,\n",
    "                lstm_dim       = 256,\n",
    "                dec_layers     = 3,\n",
    "                td_dense_dim   = 0,\n",
    "                batch_size     = 128,\n",
    "                codelayer_dim  = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "\n",
      "Model trained with dataset OPD_fingerprint that has maxlen=455 and charset=a0bMAuGpgFL=(-2l+i)5mYtVnD\\%TPI8y1S3N7OZ/#CsRUfd@Be9Eo[WKcrH4]6h^$? for 200 epochs.\n",
      "noise_std: 0.100000, lstm_dim: 256, dec_layers: 3, td_dense_dim: 0, batch_size: 128, codelayer_dim: 128, lr: 0.001000.\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 1/1000\n",
      "222/221 - 122s - loss: 0.3226 - val_loss: 0.1940\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 2/1000\n",
      "222/221 - 108s - loss: 0.1639 - val_loss: 0.1628\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 3/1000\n",
      "222/221 - 115s - loss: 0.1383 - val_loss: 0.1688\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 4/1000\n",
      "222/221 - 115s - loss: 0.1248 - val_loss: 0.3504\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 5/1000\n"
     ]
    }
   ],
   "source": [
    "model.fit(epochs              = 200,         # number of epochs\n",
    "          lr                  = 1e-3,        # initial learning rate for Adam, recommended\n",
    "          model_name          = \"./models/FP_C_\", # base name to append the checkpoints with\n",
    "          checkpoint_dir      = \"\",          # save checkpoints in the notebook's directory\n",
    "          mini_epochs         = 5,          # number of sub-epochs within an epoch to trigger lr decay\n",
    "          save_period         = 50,          # checkpoint frequency (in mini_epochs)\n",
    "          lr_decay            = True,        # whether to use exponential lr decay or not\n",
    "          sch_epoch_to_start  = 100,         # mini-epoch to start lr decay (bypassed if lr_decay=False)\n",
    "          sch_lr_init         = 1e-3,        # initial lr, should be equal to lr (bypassed if lr_decay=False)\n",
    "          sch_lr_final        = 1e-6,        # final lr before finishing training (bypassed if lr_decay=False)\n",
    "          patience            = 25)          # patience for Keras' ReduceLROnPlateau (bypassed if lr_decay=True)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
