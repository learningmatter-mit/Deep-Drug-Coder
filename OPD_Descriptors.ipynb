{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Occupy a GPU for the model to be loaded \n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# GPU ID, if occupied change to an available GPU ID listed under !nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "import h5py, ast, pickle\n",
    "\n",
    "from ddc_pub.vectorizers import SmilesVectorizer\n",
    "from ddc_pub import ddc_v3 as ddc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/OPD_Data/Desc_Training_Validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_data = df['smiles'].tolist()\n",
    "\n",
    "binmols = np.array([Chem.MolFromSmiles(x) for x in mol_data])\n",
    "sv = SmilesVectorizer()\n",
    "sv.fit(binmols)\n",
    "maxlen = sv.maxlength + 35\n",
    "charset = sv.charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr = df[['homo', 'gap', 'lumo']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset\n",
    "name = \"OPD_Descr\"\n",
    "\n",
    "dataset_info = {\"charset\": charset, \"maxlen\": maxlen, \"name\": name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model in train mode.\n",
      "Input type is 'molecular descriptors'.\n",
      "Applying scaling on input.\n",
      "Model received 12251 train samples and 1362 validation samples.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Latent_Input (InputLayer)       [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Inputs (InputLayer)     [(None, 244, 47)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "latent_to_states_model (Model)  [(None, 512), (None, 24576       Latent_Input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_model (Model)             (None, 244, 47)      5381679     Decoder_Inputs[0][0]             \n",
      "                                                                 latent_to_states_model[1][0]     \n",
      "                                                                 latent_to_states_model[1][1]     \n",
      "                                                                 latent_to_states_model[1][2]     \n",
      "                                                                 latent_to_states_model[1][3]     \n",
      "                                                                 latent_to_states_model[1][4]     \n",
      "                                                                 latent_to_states_model[1][5]     \n",
      "==================================================================================================\n",
      "Total params: 5,406,255\n",
      "Trainable params: 5,397,039\n",
      "Non-trainable params: 9,216\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialize a model\n",
    "model = ddc.DDC(x              = descr,        # input\n",
    "                y              = binmols,      # output\n",
    "                dataset_info   = dataset_info, # dataset information\n",
    "                scaling        = True,         # scale the descriptors\n",
    "                noise_std      = 0.1,          # std of the noise layer\n",
    "                lstm_dim       = 512,          # breadth of LSTM layers\n",
    "                dec_layers     = 3,            # number of decoding layers\n",
    "                batch_size     = 128)          # batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "\n",
      "Model trained with dataset OPD_Descr that has maxlen=240 and charset=]Z=g@N78Ma/+(PlC9I2s3)S6%rc0Hp5oFeO#1-\\[Bin4^$? for 2 epochs.\n",
      "noise_std: 0.100000, lstm_dim: 512, dec_layers: 3, td_dense_dim: 0, batch_size: 128, codelayer_dim: 3, lr: 0.001000.\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 1/2\n",
      "Model saved in ./models/opd_descr--01--0.4807--0.0010000.\n",
      "95/95 - 31s - loss: 0.5956 - val_loss: 0.4807\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 2/2\n",
      "Model saved in ./models/opd_descr--02--0.3681--0.0010000.\n",
      "95/95 - 32s - loss: 0.3277 - val_loss: 0.3681\n"
     ]
    }
   ],
   "source": [
    "model.fit(epochs              = 100,         # number of epochs\n",
    "          lr                  = 1e-3,        # initial learning rate for Adam, recommended\n",
    "          model_name          = \"opd_descr\", # base name to append the checkpoints with\n",
    "          checkpoint_dir      = \"./models/\", # save checkpoints in the notebook's directory\n",
    "          mini_epochs         = 10,          # number of sub-epochs within an epoch to trigger lr decay\n",
    "          save_period         = 50,          # checkpoint frequency (in mini_epochs)\n",
    "          lr_decay            = True,        # whether to use exponential lr decay or not\n",
    "          sch_epoch_to_start  = 500,         # mini-epoch to start lr decay (bypassed if lr_decay=False)\n",
    "          sch_lr_init         = 1e-3,        # initial lr, should be equal to lr (bypassed if lr_decay=False)\n",
    "          sch_lr_final        = 1e-6,        # final lr before finishing training (bypassed if lr_decay=False)\n",
    "          patience            = 25)          # patience for Keras' ReduceLROnPlateau (bypassed if lr_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:ddc_env]",
   "language": "python",
   "name": "conda-env-ddc_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
